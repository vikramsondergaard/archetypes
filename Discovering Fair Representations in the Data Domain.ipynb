{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afa87e0",
   "metadata": {},
   "source": [
    "# Discovering Fair Representations in the Data Domain\n",
    "\n",
    "Interpretability and fairness are important in machine learning. There are many situations where a machine learning model has made unfair decisions that are difficult to interpret by either its designers or the people it affects.\n",
    "\n",
    "Quadrianto *et al.* devise a method for projecting data into a \"fair\" domain. This not only identifies areas where machine learning models might discriminate, but it also eliminates these sources of discrimination. Therefore, the problem becomes one of translating data from an input space into a \"fair\" space.\n",
    "\n",
    "[Link to paper](https://arxiv.org/abs/1810.06755)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a13caff",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning models are becoming more ubiquitous in everyday life. This includes in government and business, for purposes such as hiring candidates and giving loans. Often, these systems are not *fair*: they discriminate against individuals based on characteristics that should be \"protected\". These include gender, ethnicity and ability. Previous attempts at making machine learning models fair use *latent embeddings*. These models are difficult to interpret, as these latent embeddings are the result of complex probabilistic calculations.\n",
    "\n",
    "Quadrianto *et al.* therefore devise a machine learning model that projects data into a \"fair\" domain. This means that it is easy to interpret the modifications that have been made to the data to make it fair. The way in which they achieve this is by implementing a *data-to-data translation*. This means that the data is mapped from an input domain to a target \"fair\" domain. There are several examples of this practice: Zhu *et al.*'s [CycleGAN](https://arxiv.org/abs/1703.10593) and Choi *et al.*'s [StarGAN](https://arxiv.org/abs/1711.09020) solve this problem where only *un*aligned training examples are available. The difference in this method is that *there is no data in the target domain*. That is, fair images are not readily available, and images themselves cannot be classified as fair or unfair without prior context.\n",
    "\n",
    "As an example, consider a situation where job applicants send a photo of themselves as part of the application process. A model might achieve fairness by just *translating female faces to male ones*, but this is inherently biased towards male faces. So, to achieve fairness, there are at least *two sub-problems*:\n",
    "1. How can this method be generalised to handle both image and tabular data?\n",
    "2. How can this method find a middle-ground \"fair\" representation with potentially multi-modal protected characteristic(s)?\n",
    "\n",
    "This paper solves this problem by focusing on statistical dependence/independence between \"fair\" images and protected characteristics.\n",
    "\n",
    "#### Related work\n",
    "\n",
    "This work improves on fair *but uninterpretable* machine learning models.\n",
    "\n",
    "There are *very few*, *if any* machine learning models that are fair and also retain the semantics of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b459e95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15148\\1326589404.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfnmatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import relevant libraries\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection as cross_validation\n",
    "import sys\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0f80cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0\n",
      "  Downloading tensorflow_intel-2.13.0-cp39-cp39-win_amd64.whl (276.5 MB)\n",
      "     -------------------------------------- 276.5/276.5 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting numpy<=1.24.3,>=1.22\n",
      "  Downloading numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "     --------------------------------------- 14.9/14.9 MB 81.8 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "     ---------------------------------------- 440.8/440.8 kB ? eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (21.3)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.57.0-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 69.2 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 54.6 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     --------------------------------------- 24.4/24.4 MB 59.4 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 89.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (63.4.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.24.0-cp39-cp39-win_amd64.whl (430 kB)\n",
      "     ------------------------------------- 430.6/430.6 kB 28.0 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.4)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 181.8/181.8 kB ? eta 0:00:00\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, opt-einsum, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 keras-2.13.1 libclang-16.0.6 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\u6390710\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\u6390710\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\u6390710\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\u6390710\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\u6390710\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 2.1.1 requires sentencepiece, which is not installed.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f299c",
   "metadata": {},
   "source": [
    "## Interpretability in fairness by residual decomposition\n",
    "\n",
    "Take an automated hiring algorithm as an example:\n",
    "- The input data will include photographs, work experience, education and training, personal skills, and other relevant attributes.\n",
    "- The protected characterstics can be features such as *ethnicity* or *gender*, $s^n \\in \\{A, B, C, D, ...\\}$, or *age*, $s^n \\in \\mathbb{R}$.\n",
    "\n",
    "The goal is then to train a classifier $f$ that predicts whether the company should hire the given person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759cd27",
   "metadata": {},
   "source": [
    "### Fairness definitions\n",
    "\n",
    "Existing papers on mathematical definitions on fairness abound, including using [disparate impact](https://arxiv.org/abs/1610.07524) and [formalised fairness conditions](https://arxiv.org/abs/1609.05807). The focus in this work is on *equality of opportunity*. This requires that $f$ and the protected characteristic $s$ be independent, conditional on the label being positive. That is, $f \\perp \\! \\! \\! \\perp s | y = +1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18258eff",
   "metadata": {},
   "source": [
    "### Residual decomposition\n",
    "\n",
    "Goals for the data representation $\\tilde{\\mathbf{x}}^n$, given an input $\\mathbf{x}^n$:\n",
    "1. It can predict the output label $y^n$\n",
    "2. It is fair according to some criterion with respect to $s$, the protected characteristic\n",
    "3. It lies in the same space as $\\mathbf{x}^n$. So $\\tilde{\\mathbf{x}}^n \\in \\mathcal{X}$.\n",
    "\n",
    "The third goal is especially important as it provides a starting point for keeping the same *semantic meaning* between $\\mathbf{x}^n$ and $\\tilde{\\mathbf{x}}^n$. Preserving semantic meaning can only be done thanks to *observational data*: this allows one to inspect how fairness criteria, protected characteristics and classification accuracy work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b036f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_features_by_prefixes(df, prefixes):\n",
    "    res = []\n",
    "    for name in df.columns:\n",
    "        filtered = False\n",
    "        for pref in prefixes:\n",
    "            if name.startswith(pref):\n",
    "                filtered = True\n",
    "                break\n",
    "        if not filtered:\n",
    "            res.append(name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e48f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_feature_names(df, data_name, feature_split):\n",
    "    if data_name == \"adult\":\n",
    "        if feature_split == \"sex_salary\":\n",
    "            x_features = _filter_features_by_prefixes(df, ['sex', 'salary'])\n",
    "            s_features = ['sex_Male']\n",
    "            y_features = ['salary_>50K']\n",
    "        elif feature_split == \"race_salary\":\n",
    "            x_features = _filter_features_by_prefixes(\n",
    "                df, ['race', 'salary'])\n",
    "            s_features = [\n",
    "                    'race_Amer-Indian-Eskimo',\n",
    "                    'race_Asian-Pac-Islander',\n",
    "                    'race_Black',\n",
    "                    'race_White',\n",
    "                ]\n",
    "            y_features = ['salary_>50K']\n",
    "        elif feature_split == \"sex-race_salary\":\n",
    "            x_features = _filter_features_by_prefixes(\n",
    "                df, ['sex', 'race', 'salary'])\n",
    "            s_features = [\n",
    "                    'sex_Male',\n",
    "                    'race_Amer-Indian-Eskimo',\n",
    "                    'race_Asian-Pac-Islander',\n",
    "                    'race_Black',\n",
    "                    'race_White',\n",
    "                ]\n",
    "            y_features = ['salary_>50K']\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    elif data_name== \"nypd\":\n",
    "        if feature_split== \"sex_possession\":\n",
    "            x_features = _filter_features_by_prefixes(df, ['sex', 'possession'])\n",
    "            s_features = ['sex_M']\n",
    "            y_features = ['possession']\n",
    "        elif feature_split== \"sex-race_possession\":\n",
    "            x_features = _filter_features_by_prefixes(\n",
    "                df, ['sex', 'race', 'possession'])\n",
    "            s_features = [\n",
    "                'sex_M',\n",
    "                'sex_F',\n",
    "                'sex_Z',\n",
    "                'race_A',\n",
    "                'race_B',\n",
    "                'race_I',\n",
    "                'race_P',\n",
    "                'race_Q',\n",
    "                'race_U',\n",
    "                'race_W',\n",
    "                'race_Z',\n",
    "            ]\n",
    "            y_features = ['possession']\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    elif data_name == \"australian_ad_observatory\":\n",
    "        if feature_split == \"Observer Postcode_Targeted\":\n",
    "            x_features = _filter_features_by_prefixes(df, ['Observer Postcode', 'Targeted'])\n",
    "            s_features = ['Observer Postcode']\n",
    "            y_features = ['Targeted']\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return x_features, s_features, y_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "badf7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_features(df, data_name, feature_split):\n",
    "    x_features, s_features, y_features = _split_feature_names(\n",
    "        df, data_name, feature_split)\n",
    "\n",
    "    # allowing to go back from one hot encoded features to categorical features\n",
    "    if data_name==\"adult\":\n",
    "        SORTED_FEATURES_NAMES = [\n",
    "                'age',\n",
    "                'education-num',\n",
    "                'capital-gain',\n",
    "                'capital-loss',\n",
    "                'hours-per-week',\n",
    "                'workclass',\n",
    "                'education',\n",
    "                'marital-status',\n",
    "                'occupation',\n",
    "                'relationship',\n",
    "                'race',\n",
    "                'sex',\n",
    "                'native-country',\n",
    "                'salary'   \n",
    "            ]\n",
    "\n",
    "    features = OrderedDict()\n",
    "    for i in range(len(SORTED_FEATURES_NAMES)):\n",
    "        features[SORTED_FEATURES_NAMES[i]] = [not re.match(SORTED_FEATURES_NAMES[i],values)==None for values in x_features]\n",
    "\n",
    "    # fixing the education to not count education-num\n",
    "    features['education'][1] = False\n",
    "    x = df[x_features].values.astype(float)\n",
    "    s = df[s_features].values.astype(float)\n",
    "    y = df[y_features].values.astype(float)\n",
    "\n",
    "    return x, s, y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1fe7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(scaler_class, train, valid, test):\n",
    "    if scaler_class is None:\n",
    "        return [train, valid, test]\n",
    "\n",
    "    scaler = scaler_class()\n",
    "    scalerobj = scaler.fit(np.concatenate((np.concatenate((train,valid),axis=0),test),axis=0))\n",
    "    train_scaled = scalerobj.transform(train)\n",
    "    valid_scaled = scalerobj.transform(valid)\n",
    "    test_scaled = scalerobj.transform(test)\n",
    "    return [train_scaled, valid_scaled, test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path, test_path, validation_size, random_state,\n",
    "                 data_name, feature_split, remake_test=False, test_size=None,\n",
    "                 input_scaler=StandardScaler, sensitive_scaler=None):\n",
    "    df_train_raw = pd.read_csv(train_path, engine='c')\n",
    "    df_test_raw = pd.read_csv(test_path)\n",
    "\n",
    "    if remake_test:\n",
    "        if test_size is None:\n",
    "            test_size = df_test_raw.shape[0]\n",
    "\n",
    "        df_full = pd.concat([df_train_raw, df_test_raw])\n",
    "        df_full_shuffled = df_full.sample(frac=1, random_state=random_state)\n",
    "\n",
    "        df_train_valid = df_full_shuffled[:-test_size]\n",
    "        df_test = df_full_shuffled[-test_size:]\n",
    "\n",
    "    else:\n",
    "        if test_size is not None:\n",
    "            raise ValueError(\"Changing test size is only possible \"\n",
    "                             \"if remake_test is True.\")\n",
    "\n",
    "        df_train_valid = df_train_raw.sample(frac=1, random_state=random_state)\n",
    "        df_test = df_test_raw\n",
    "\n",
    "    df_train = df_train_valid[:-validation_size]\n",
    "    df_valid = df_train_valid[-validation_size:]\n",
    "\n",
    "    x_train, s_train, y_train, cat_features = _split_features(df_train, data_name, \n",
    "                                                feature_split)\n",
    "    x_valid, s_valid, y_valid,_ = _split_features(df_valid, data_name, \n",
    "                                                feature_split)\n",
    "    x_test, s_test, y_test,_ = _split_features(df_test, data_name, \n",
    "                                             feature_split)\n",
    "\n",
    "    x_train, x_valid, x_test = scale(\n",
    "        input_scaler, x_train, x_valid, x_test)\n",
    "\n",
    "    s_train, s_valid, s_test = scale(\n",
    "        sensitive_scaler, s_train, s_valid, s_test)\n",
    "\n",
    "    data_train = x_train, s_train, y_train\n",
    "    data_valid = x_valid, s_valid, y_valid\n",
    "    data_test = x_test, s_test, y_test\n",
    "\n",
    "    return data_train, data_valid, data_test, cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67869f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median(v):\n",
    "    v = tf.reshape(v, [-1])\n",
    "    m = v.get_shape()[0] // 2\n",
    "    return tf.nn.top_k(v, m).values[m - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ff233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_median_pairwise_euclidean_distance(X):\n",
    "    XX = tf.matmul(X, X, transpose_b=True)\n",
    "    X_sqnorms = tf.diag_part(XX)\n",
    "    r = lambda x: tf.expand_dims(x, 0)\n",
    "    c = lambda x: tf.expand_dims(x, 1)\n",
    "    pair_dist = (-2 * XX + c(X_sqnorms) + r(X_sqnorms))\n",
    "    pair_dist = tf.nn.relu(pair_dist)\n",
    "    sq_dist = tf.sqrt(pair_dist)\n",
    "    med_sqrt = get_median(sq_dist)\n",
    "    return med_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_euclidean_distance(data):\n",
    "    X, s, y = data\n",
    "\n",
    "    XX = np.dot(X, X.T)\n",
    "    X_sqnorms = np.diag(XX)\n",
    "    r = lambda x: np.expand_dims(x, 0)\n",
    "    c = lambda x: np.expand_dims(x, 1)\n",
    "    pair_dist = (-2 * XX + c(X_sqnorms) + r(X_sqnorms))\n",
    "    pair_dist[pair_dist < 0] = 0\n",
    "    return np.sqrt(pair_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab62dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_bn_relu(inp, units, deploy):\n",
    "    units = tf.layers.dense(\n",
    "        inp, units, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.uniform_unit_scaling_initializer(seed=888))\n",
    "    return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_time_HSIC(data_first, data_second, sigma_first, sigma_second):\n",
    "    XX = tf.matmul(data_first, data_first, transpose_b=True)\n",
    "    YY = tf.matmul(data_second, data_second, transpose_b=True)\n",
    "    X_sqnorms = tf.diag_part(XX)\n",
    "    Y_sqnorms = tf.diag_part(YY)\n",
    "\n",
    "    r = lambda x: tf.expand_dims(x, 0)\n",
    "    c = lambda x: tf.expand_dims(x, 1)\n",
    "\n",
    "    gamma_first = 1. / (2 * sigma_first**2)\n",
    "    gamma_second = 1. / (2 * sigma_second**2)\n",
    "    # use the second binomial formula\n",
    "    Kernel_XX = tf.exp(-gamma_first * (-2 * XX + c(X_sqnorms) + r(X_sqnorms)))\n",
    "    Kernel_YY = tf.exp(-gamma_second * (-2 * YY + c(Y_sqnorms) + r(Y_sqnorms)))\n",
    "\n",
    "    Kernel_XX_mean = tf.reduce_mean(Kernel_XX, 0, keep_dims=True)\n",
    "    Kernel_YY_mean = tf.reduce_mean(Kernel_YY, 0, keep_dims=True)\n",
    "\n",
    "    HK = Kernel_XX - Kernel_XX_mean\n",
    "    HL = Kernel_YY - Kernel_YY_mean\n",
    "\n",
    "    n = tf.cast(tf.shape(Kernel_YY)[0], tf.float32)\n",
    "    HKf = HK / (n - 1)\n",
    "    HLf = HL / (n - 1)\n",
    "\n",
    "    # biased estimate\n",
    "    hsic = tf.trace(tf.matmul(HKf, HLf))\n",
    "    return hsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6156db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_time_MMD(data_first, data_second, data_third, data_fourth, sigma,\n",
    "    data_fifth=None, data_sixth=None):\n",
    "\n",
    "    # kernel width\n",
    "    gamma = 1 / (2 * sigma**2)\n",
    "\n",
    "    # handles X and S first\n",
    "    XX_1 = tf.matmul(data_first, data_first, transpose_b=True)\n",
    "    XX_2 = tf.matmul(data_third, data_third, transpose_b=True)\n",
    "\n",
    "    YY_1 = tf.matmul(data_second, data_second, transpose_b=True)\n",
    "    YY_2 = tf.matmul(data_fourth, data_fourth, transpose_b=True)\n",
    "\n",
    "    X_12 = tf.matmul(data_first, data_third, transpose_b=True)\n",
    "    Y_12 = tf.matmul(data_second, data_fourth, transpose_b=True)\n",
    "\n",
    "    X_sqnorms_1 = tf.diag_part(XX_1)\n",
    "    X_sqnorms_2 = tf.diag_part(XX_2)\n",
    "    Y_sqnorms_1 = tf.diag_part(YY_1)\n",
    "    Y_sqnorms_2 = tf.diag_part(YY_2)\n",
    "\n",
    "    r = lambda x: tf.expand_dims(x, 0)\n",
    "    c = lambda x: tf.expand_dims(x, 1)\n",
    "\n",
    "    # use the second binomial formula\n",
    "    Kernel_XX_1 = tf.exp(-gamma * (-2 * XX_1 + c(X_sqnorms_1) + r(X_sqnorms_1)))\n",
    "    Kernel_XX_2 = tf.exp(-gamma * (-2 * XX_2 + c(X_sqnorms_2) + r(X_sqnorms_2)))\n",
    "\n",
    "    Kernel_YY_1 = tf.exp(-gamma * (-2 * YY_1 + c(Y_sqnorms_1) + r(Y_sqnorms_1)))\n",
    "    Kernel_YY_2 = tf.exp(-gamma * (-2 * YY_2 + c(Y_sqnorms_2) + r(Y_sqnorms_2)))\n",
    "\n",
    "    Kernel_X_12 = tf.exp(-gamma * (-2 * X_12 + c(X_sqnorms_1) + r(X_sqnorms_2)))\n",
    "    Kernel_Y_12 = tf.exp(-gamma * (-2 * Y_12 + c(Y_sqnorms_1) + r(Y_sqnorms_2)))\n",
    "\n",
    "    # then handles the conditioning variable, Y\n",
    "    if data_fifth==None:\n",
    "        # use product kernels, a Hadamard product between the original kernel matrices for each variable\n",
    "        Kernel_1 = tf.multiply(Kernel_XX_1,Kernel_YY_1)\n",
    "        Kernel_2 = tf.multiply(Kernel_XX_2,Kernel_YY_2)\n",
    "        Kernel_12 = tf.multiply(Kernel_X_12,Kernel_Y_12)\n",
    "\n",
    "    else:\n",
    "        # use product kernels, a Hadamard product between the original kernel matrices for each variable\n",
    "        ZZ_1 = tf.matmul(data_fifth, data_fifth, transpose_b=True)\n",
    "        ZZ_2 = tf.matmul(data_sixth, data_sixth, transpose_b=True)\n",
    "        Z_12 = tf.matmul(data_fifth, data_sixth, transpose_b=True)\n",
    "\n",
    "        Z_sqnorms_1 = tf.diag_part(ZZ_1)\n",
    "        Z_sqnorms_2 = tf.diag_part(ZZ_2)\n",
    "\n",
    "        Kernel_ZZ_1 = tf.exp(-gamma * (-2 * ZZ_1 + c(Z_sqnorms_1) + r(Z_sqnorms_1)))\n",
    "        Kernel_ZZ_2 = tf.exp(-gamma * (-2 * ZZ_2 + c(Z_sqnorms_2) + r(Z_sqnorms_2)))\n",
    "        Kernel_Z_12 = tf.exp(-gamma * (-2 * Z_12 + c(Z_sqnorms_1) + r(Z_sqnorms_2)))\n",
    "\n",
    "        Kernel_1 = tf.multiply(Kernel_XX_1,Kernel_YY_1)\n",
    "        Kernel_1 = tf.multiply(Kernel_1,Kernel_ZZ_1)\n",
    "\n",
    "        Kernel_2 = tf.multiply(Kernel_XX_2,Kernel_YY_2)\n",
    "        Kernel_2 = tf.multiply(Kernel_2,Kernel_ZZ_2)\n",
    "\n",
    "        Kernel_12 = tf.multiply(Kernel_X_12,Kernel_Y_12)\n",
    "        Kernel_12 = tf.multiply(Kernel_12,Kernel_Z_12)\n",
    "\n",
    "    m = tf.cast(tf.shape(XX_1)[0],tf.float32)\n",
    "    n = tf.cast(tf.shape(XX_2)[0],tf.float32)\n",
    "\n",
    "    mmd2 = (tf.reduce_sum(Kernel_1) / (m * m)\n",
    "          + tf.reduce_sum(Kernel_2) / (n * n)\n",
    "          - 2 * tf.reduce_sum(Kernel_12) / (m * n))\n",
    "    return 4.0*mmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_marginal_data(*arrays, random_seed=523423, model_config):\n",
    "    # X is conditional independence of S given Y\n",
    "    # X,S,Y; permute the S according to the Y\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    # permute all of them once\n",
    "    n_obs = len(arrays[0])\n",
    "    rperm = np.random.permutation(n_obs)\n",
    "    res = []\n",
    "    for arr in arrays:\n",
    "        res.append(arr[rperm,])\n",
    "\n",
    "    # now, permute according to the conditional independency\n",
    "    if model_config['equalized_odds']:\n",
    "        # equalized odds\n",
    "        df = pd.DataFrame(res[1])\n",
    "        df = df.groupby(res[2].flatten(), group_keys=False).transform(np.random.permutation)\n",
    "        res[1] = df.as_matrix()\n",
    "    else:\n",
    "        # equality of opportunity\n",
    "        rperm = np.random.permutation(n_obs)\n",
    "        res[1] = res[1][rperm,]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45995444",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Performance Metric ######\n",
    "\n",
    "\n",
    "def compute_accuracy_multi_pvalue(Y, predictions, Xcontrol):\n",
    "    Xcontrol = [np.argmax(x) for x in Xcontrol]\n",
    "    correct = np.sum(Y == predictions)\n",
    "    acc = correct * 1. / Y.shape[0]\n",
    "    acc_sensitive = np.zeros(np.unique(Xcontrol).shape[0])\n",
    "    ii = 0\n",
    "    for v in np.unique(Xcontrol):\n",
    "        idx_ = Xcontrol == v\n",
    "        acc_sensitive[ii] = np.sum(Y[idx_] == predictions[idx_,]) / (np.sum(idx_) * 1.)\n",
    "        ii = ii + 1\n",
    "    return acc, acc_sensitive  # , pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_pvalue(Y, predictions, Xcontrol):\n",
    "    correct = np.sum(Y == predictions)\n",
    "    acc = correct * 1. / Y.shape[0]\n",
    "    acc_sensitive = np.zeros(np.unique(Xcontrol).shape[0])\n",
    "    ii = 0\n",
    "    for v in np.unique(Xcontrol):\n",
    "        idx_ = Xcontrol == v\n",
    "        acc_sensitive[ii] = np.sum(Y[idx_] == predictions[idx_,]) / (np.sum(idx_) * 1.)\n",
    "        ii = ii + 1\n",
    "    return acc, acc_sensitive  # , pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee893db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multi_fpr_fnr(Y, predictions, Xcontrol):\n",
    "    Xcontrol = [np.argmax(x) for x in Xcontrol]\n",
    "    fp = np.sum(np.logical_and(Y == 0.0, predictions == +1.0))  # something which is -ve but is misclassified as +ve\n",
    "    fn = np.sum(np.logical_and(Y == +1.0, predictions == 0.0))  # something which is +ve but is misclassified as -ve\n",
    "    tp = np.sum(\n",
    "        np.logical_and(Y == +1.0, predictions == +1.0))  # something which is +ve AND is correctly classified as +ve\n",
    "    tn = np.sum(\n",
    "        np.logical_and(Y == 0.0, predictions == 0.0))  # something which is -ve AND is correctly classified as -ve\n",
    "    fpr_all = np.float(fp) / np.float(fp + tn)\n",
    "    fnr_all = np.float(fn) / np.float(fn + tp)\n",
    "    tpr_all = np.float(tp) / np.float(tp + fn)\n",
    "    tnr_all = np.float(tn) / np.float(tn + fp)\n",
    "\n",
    "    fpr_fnr_tpr_sensitive = np.zeros((4, np.unique(Xcontrol).shape[0]))  # ~~~ I changed this from 3 so add tnr\n",
    "    ii = 0\n",
    "    for v in np.unique(Xcontrol):\n",
    "        idx_ = Xcontrol == v\n",
    "        fp = np.sum(np.logical_and(Y[idx_] == 0.0,\n",
    "                                   predictions[idx_] == +1.0))  # something which is -ve but is misclassified as +ve\n",
    "        fn = np.sum(np.logical_and(Y[idx_] == +1.0,\n",
    "                                   predictions[idx_] == 0.0))  # something which is +ve but is misclassified as -ve\n",
    "        tp = np.sum(np.logical_and(Y[idx_] == +1.0, predictions[\n",
    "            idx_] == +1.0))  # something which is +ve AND is correctly classified as +ve\n",
    "        tn = np.sum(np.logical_and(Y[idx_] == 0.0, predictions[\n",
    "            idx_] == 0.0))  # something which is -ve AND is correctly classified as -ve\n",
    "        fpr = np.float(fp) / np.float(fp + tn)\n",
    "        fnr = np.float(fn) / np.float(fn + tp)\n",
    "        tpr = np.float(tp) / np.float(tp + fn)\n",
    "        tnr = np.float(tn) / np.float(tn + fp)\n",
    "        fpr_fnr_tpr_sensitive[0, ii] = fpr\n",
    "        fpr_fnr_tpr_sensitive[1, ii] = fnr\n",
    "        fpr_fnr_tpr_sensitive[2, ii] = tpr\n",
    "        fpr_fnr_tpr_sensitive[3, ii] = tnr\n",
    "        ii = ii + 1\n",
    "    return fpr_all, fnr_all, fpr_fnr_tpr_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fpr_fnr(Y, predictions, Xcontrol):\n",
    "    fp = np.sum(np.logical_and(Y == 0.0, predictions == +1.0))  # something which is -ve but is misclassified as +ve\n",
    "    fn = np.sum(np.logical_and(Y == +1.0, predictions == 0.0))  # something which is +ve but is misclassified as -ve\n",
    "    tp = np.sum(\n",
    "        np.logical_and(Y == +1.0, predictions == +1.0))  # something which is +ve AND is correctly classified as +ve\n",
    "    tn = np.sum(\n",
    "        np.logical_and(Y == 0.0, predictions == 0.0))  # something which is -ve AND is correctly classified as -ve\n",
    "    fpr_all = np.float(fp) / np.float(fp + tn)\n",
    "    fnr_all = np.float(fn) / np.float(fn + tp)\n",
    "    tpr_all = np.float(tp) / np.float(tp + fn)\n",
    "    tnr_all = np.float(tn) / np.float(tn + fp)\n",
    "\n",
    "    fpr_fnr_tpr_sensitive = np.zeros((4, np.unique(Xcontrol).shape[0]))\n",
    "    ii = 0\n",
    "    for v in np.unique(Xcontrol):\n",
    "        idx_ = Xcontrol == v\n",
    "        fp = np.sum(np.logical_and(Y[idx_] == 0.0,\n",
    "                                   predictions[idx_] == +1.0))  # something which is -ve but is misclassified as +ve\n",
    "        fn = np.sum(np.logical_and(Y[idx_] == +1.0,\n",
    "                                   predictions[idx_] == 0.0))  # something which is +ve but is misclassified as -ve\n",
    "        tp = np.sum(np.logical_and(Y[idx_] == +1.0, predictions[\n",
    "            idx_] == +1.0))  # something which is +ve AND is correctly classified as +ve\n",
    "        tn = np.sum(np.logical_and(Y[idx_] == 0.0, predictions[\n",
    "            idx_] == 0.0))  # something which is -ve AND is correctly classified as -ve\n",
    "        fpr = np.float(fp) / np.float(fp + tn)\n",
    "        fnr = np.float(fn) / np.float(fn + tp)\n",
    "        tpr = np.float(tp) / np.float(tp + fn)\n",
    "        tnr = np.float(tn) / np.float(tn + fp)\n",
    "        fpr_fnr_tpr_sensitive[0, ii] = fpr\n",
    "        fpr_fnr_tpr_sensitive[1, ii] = fnr\n",
    "        fpr_fnr_tpr_sensitive[2, ii] = tpr\n",
    "        fpr_fnr_tpr_sensitive[3, ii] = tnr\n",
    "        ii = ii + 1\n",
    "    return fpr_all, fnr_all, fpr_fnr_tpr_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "class Model:\n",
    "    def __init__(self, features_size, protected_size, target_size, features_names, rff_map, rff_map_sens, to_deploy,\n",
    "                 code_size,\n",
    "                 encoder_hidden_sizes, decoder_hidden_sizes,\n",
    "                 predictor_hidden_sizes,\n",
    "                 hsic_cost_weight, pred_cost_weight, dec_cost_weight, rff_samples, equalized_odds, device):\n",
    "        if not to_deploy:\n",
    "            self.init_network(features_size, protected_size, target_size, features_names, code_size,\n",
    "                              encoder_hidden_sizes, decoder_hidden_sizes,\n",
    "                              predictor_hidden_sizes, to_deploy, device)\n",
    "            self.init_training(hsic_cost_weight, pred_cost_weight, dec_cost_weight, rff_map, rff_map_sens,\n",
    "                               equalized_odds)\n",
    "            self.init_logging(hsic_cost_weight, pred_cost_weight, dec_cost_weight)\n",
    "        else:\n",
    "            self.init_network(features_size, protected_size, target_size, features_names, code_size,\n",
    "                              encoder_hidden_sizes, decoder_hidden_sizes,\n",
    "                              predictor_hidden_sizes, to_deploy, device)\n",
    "\n",
    "    def init_network(self, features_size, protected_size, target_size, features,\n",
    "                     code_size, encoder_hidden_sizes, decoder_hidden_sizes,\n",
    "                     predictor_hidden_sizes, deploy, device):\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, [None, features_size], name=\"x\")\n",
    "        self.s = tf.placeholder(tf.float32, [None, protected_size], name=\"s\")\n",
    "        self.y = tf.placeholder(tf.float32, [None, target_size], name=\"y\")\n",
    "        self.x_marg = tf.placeholder(tf.float32, [None, features_size], name=\"x_marg\")\n",
    "        self.s_marg = tf.placeholder(tf.float32, [None, protected_size], name=\"s_marg\")\n",
    "        self.y_marg = tf.placeholder(tf.float32, [None, target_size], name=\"y_marg\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        with tf.device(device):\n",
    "            with tf.variable_scope('encoder'):\n",
    "                prev_layer = self.x\n",
    "                for size in encoder_hidden_sizes:\n",
    "                    prev_layer = dense_bn_relu(prev_layer, size, deploy)\n",
    "                tmp = tf.layers.dense(\n",
    "                    prev_layer, code_size, activation=None,\n",
    "                    kernel_initializer=tf.uniform_unit_scaling_initializer(\n",
    "                        seed=888))\n",
    "                self.encoded = tf.nn.dropout(tmp, keep_prob=self.keep_prob, seed=888)\n",
    "\n",
    "            with tf.variable_scope('decoder'):\n",
    "                prev_layer = self.encoded\n",
    "                for size in decoder_hidden_sizes:\n",
    "                    prev_layer = dense_bn_relu(prev_layer, size, deploy)\n",
    "                # take into account the structure of our features\n",
    "                keys_f = features.keys()\n",
    "                ii = 0\n",
    "                for key_f in keys_f:\n",
    "                    if sum(features[key_f]) > 0 and sum(features[key_f]) == 1:\n",
    "                        inc_unit = tf.layers.dense(\n",
    "                            prev_layer, sum(features[key_f]), activation=None,\n",
    "                            kernel_initializer=tf.uniform_unit_scaling_initializer(seed=888))\n",
    "                        if ii == 0:\n",
    "                            self.decoded = inc_unit\n",
    "                        else:\n",
    "                            self.decoded = tf.concat([self.decoded, inc_unit], axis=1)\n",
    "                    elif sum(features[key_f]) > 1:\n",
    "                        inc_unit = tf.layers.dense(\n",
    "                            prev_layer, sum(features[key_f]), activation=tf.nn.softmax,\n",
    "                            kernel_initializer=tf.uniform_unit_scaling_initializer(seed=888))\n",
    "                        if ii == 0:\n",
    "                            if deploy:  # use one hot encoding at the deployment\n",
    "                                self.decoded = tf.one_hot(tf.argmax(inc_unit, dimension=1), depth=sum(features[key_f]))\n",
    "                            else:  # use soft outputs for learning\n",
    "                                self.decoded = inc_unit\n",
    "                        else:\n",
    "                            if deploy:  # use one hot encoding at the deployment\n",
    "                                self.decoded = tf.concat([self.decoded, tf.one_hot(tf.argmax(inc_unit, dimension=1),\n",
    "                                                                                   depth=sum(features[key_f]))], axis=1)\n",
    "                            else:  # use soft outputs for learning\n",
    "                                self.decoded = tf.concat([self.decoded, inc_unit],\n",
    "                                                         axis=1)\n",
    "                    ii += 1\n",
    "\n",
    "            with tf.variable_scope('encoder', reuse=True):\n",
    "                prev_layer = self.x_marg\n",
    "                for size in encoder_hidden_sizes:\n",
    "                    prev_layer = dense_bn_relu(prev_layer, size, deploy)\n",
    "                tmp = tf.layers.dense(\n",
    "                    prev_layer, code_size, activation=None,\n",
    "                    kernel_initializer=tf.uniform_unit_scaling_initializer(\n",
    "                        seed=888))\n",
    "                self.encoded_marginal = tf.nn.dropout(tmp, keep_prob=self.keep_prob, seed=888)\n",
    "\n",
    "            with tf.variable_scope('decoder', reuse=True):\n",
    "                prev_layer = self.encoded_marginal\n",
    "                for size in decoder_hidden_sizes:\n",
    "                    prev_layer = dense_bn_relu(prev_layer, size, deploy)\n",
    "                # take into account the structure of our features\n",
    "                keys_f = features.keys()\n",
    "                ii = 0\n",
    "                for key_f in keys_f:\n",
    "                    if sum(features[key_f]) > 0 and sum(features[key_f]) == 1:\n",
    "                        inc_unit = tf.layers.dense(\n",
    "                            prev_layer, sum(features[key_f]), activation=None,\n",
    "                            kernel_initializer=tf.uniform_unit_scaling_initializer(seed=888))\n",
    "                        if ii == 0:\n",
    "                            self.decoded_marginal = inc_unit\n",
    "                        else:\n",
    "                            self.decoded_marginal = tf.concat([self.decoded_marginal, inc_unit], axis=1)\n",
    "                    elif sum(features[key_f]) > 1:\n",
    "                        inc_unit = tf.layers.dense(\n",
    "                            prev_layer, sum(features[key_f]), activation=tf.nn.softmax,\n",
    "                            kernel_initializer=tf.uniform_unit_scaling_initializer(seed=888))\n",
    "                        if ii == 0:\n",
    "                            if deploy:  # use one hot encoding at the deployment\n",
    "                                self.decoded_marginal = tf.one_hot(tf.argmax(inc_unit, dimension=1),\n",
    "                                                                   depth=sum(features[key_f]))\n",
    "                            else:  # use soft outputs for learning\n",
    "                                self.decoded_marginal = inc_unit\n",
    "                        else:\n",
    "                            if deploy:  # use one hot encoding at the deployment\n",
    "                                self.decoded_marginal = tf.concat([self.decoded_marginal,\n",
    "                                                                   tf.one_hot(tf.argmax(inc_unit, dimension=1),\n",
    "                                                                              depth=sum(features[key_f]))], axis=1)\n",
    "                            else:  # use soft outputs for learning\n",
    "                                self.decoded_marginal = tf.concat([self.decoded_marginal, inc_unit],\n",
    "                                                                  axis=1)\n",
    "                    ii += 1\n",
    "\n",
    "            with tf.variable_scope('predictor'):\n",
    "                prev_layer = self.decoded\n",
    "                for size in predictor_hidden_sizes:\n",
    "                    prev_layer = dense_bn_relu(prev_layer, size, deploy)\n",
    "                self.y_logit = tf.layers.dense(\n",
    "                    prev_layer, target_size, activation=None,\n",
    "                    kernel_initializer=tf.uniform_unit_scaling_initializer(seed=888))\n",
    "                self.y_prob = tf.nn.sigmoid(self.y_logit)\n",
    "                self.y_pred = tf.cast(\n",
    "                    tf.greater(self.y_prob, 0.5), tf.int32)\n",
    "\n",
    "    def init_training(self, hsic_cost_weight, pred_cost_weight, dec_cost_weight,\n",
    "                      rff_map, rff_map_sens, equalized_odds):\n",
    "\n",
    "        self.y_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=self.y_logit, labels=tf.cast(self.y, tf.float32)))\n",
    "\n",
    "        # compute MMD and decoder loss in the feature space\n",
    "        # via random Fourier features machinery\n",
    "        x_map = rff_map.map(self.x)\n",
    "        x_marginal_map = rff_map.map(self.x_marg)\n",
    "        decoded_map = rff_map.map(self.decoded)\n",
    "        decoded_marginal_map = rff_map.map(self.decoded_marginal)\n",
    "        sens_map = self.s\n",
    "        sens_marginal_map = self.s_marg\n",
    "\n",
    "        self.decoder_loss = tf.reduce_mean(\n",
    "            tf.nn.l2_loss(self.x - self.decoded))\n",
    "\n",
    "        if equalized_odds:\n",
    "            # EQUALIZED ODDS\n",
    "            print(\"Equalized Odds Criterion\")\n",
    "            self.cycling_cost = -quadratic_time_MMD(x_map - decoded_map,\n",
    "                                                    sens_map, x_marginal_map - decoded_marginal_map,\n",
    "                                                    sens_marginal_map, 0.2, self.y, self.y_marg)\n",
    "        else:\n",
    "            # EQUALITY of OPPORTUNITY\n",
    "            print(\"Equal Opportunity Criterion\")\n",
    "            mask = tf.equal(tf.squeeze(self.y),1)\n",
    "            x_map_pos = tf.gather_nd(x_map, tf.where(mask))\n",
    "            x_marginal_map_pos = tf.gather_nd(x_marginal_map, tf.where(mask))\n",
    "            decoded_map_pos = tf.gather_nd(decoded_map, tf.where(mask))\n",
    "            decoded_marginal_map_pos = tf.gather_nd(decoded_marginal_map, tf.where(mask))\n",
    "            sens_map_pos = tf.gather_nd(sens_map, tf.where(mask))\n",
    "            sens_marginal_map_pos = tf.gather_nd(sens_marginal_map, tf.where(mask))\n",
    "\n",
    "            self.cycling_cost = -(quadratic_time_HSIC(x_map_pos - decoded_map_pos, sens_map_pos, tf.sqrt(tf.constant([0.5])), tf.constant([1.])))\n",
    "\n",
    "            self.hsic_cost = quadratic_time_HSIC(decoded_map_pos, sens_map_pos, tf.sqrt(tf.constant([0.5])), tf.constant([1.]))\n",
    "\n",
    "        self.pred_loss = (hsic_cost_weight * self.cycling_cost +\n",
    "                          (hsic_cost_weight * self.hsic_cost) +\n",
    "                          dec_cost_weight * self.decoder_loss +\n",
    "                          pred_cost_weight * self.y_cost)\n",
    "\n",
    "        pred_vars = (\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'encoder') +\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'decoder') +\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'predictor')\n",
    "        )\n",
    "        self.train_pred = tf.train.AdamOptimizer().minimize(\n",
    "            self.pred_loss, var_list=pred_vars)\n",
    "\n",
    "    def init_logging(self, hsic_cost_weight, pred_cost_weight, dec_cost_weight):\n",
    "        # which variables to log\n",
    "        tf.summary.scalar(\"decoder_loss\", self.decoder_loss)\n",
    "        tf.summary.scalar(\"decoder_loss_with_weight\", dec_cost_weight * self.decoder_loss)\n",
    "        tf.summary.scalar(\"pred_loss\", self.pred_loss)\n",
    "        tf.summary.scalar(\"y_cost\", self.y_cost)\n",
    "        tf.summary.scalar(\"y_cost_with_weight\", pred_cost_weight * self.y_cost)\n",
    "        tf.summary.scalar(\"cycling_cost\", self.cycling_cost)\n",
    "        tf.summary.scalar(\"cycling_cost_with_weight\", hsic_cost_weight * self.cycling_cost)\n",
    "        tf.summary.scalar(\"hsic_cost\", self.hsic_cost)\n",
    "        tf.summary.scalar(\"hsic_cost_with_weight\", hsic_cost_weight * self.hsic_cost)\n",
    "\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step',\n",
    "                                       trainable=False, dtype=tf.int32)\n",
    "        self.inc_step = tf.assign(self.global_step, self.global_step + 1)\n",
    "\n",
    "        self.global_iteration = tf.Variable(0, name='global_iteration',\n",
    "                                            trainable=False, dtype=tf.int32)\n",
    "        self.inc_iteration = tf.assign(self.global_iteration,\n",
    "                                       self.global_iteration + 1)\n",
    "\n",
    "    def fit(self, train_data, train_data_marginal, valid_data, valid_data_marginal, SEED_NUM, logs_dir, verbose,\n",
    "            tf_config,\n",
    "            n_iterations, batch_size, model_save_iterations, report_iterations,\n",
    "            pred_steps_per_iteration,\n",
    "            init_random_seed):\n",
    "\n",
    "        X_train, s_train, y_train = train_data\n",
    "        X_valid, s_valid, y_valid = valid_data\n",
    "\n",
    "        X_train_marginal, s_train_marginal, y_train_marginal = train_data_marginal\n",
    "        X_valid_marginal, s_valid_marginal, y_valid_marginal = valid_data_marginal\n",
    "\n",
    "        model_saver = tf.train.Saver(max_to_keep=None)\n",
    "        models_dir = logs_dir + '/models_{}/'.format(SEED_NUM)\n",
    "        last_exists = True\n",
    "        last_path = models_dir + 'last.session'\n",
    "        if not os.path.exists(models_dir):\n",
    "            os.makedirs(models_dir)\n",
    "            last_exists = False\n",
    "\n",
    "        with tf.Session(config=tf_config) as sess:\n",
    "            np.random.seed(init_random_seed)\n",
    "            tf.set_random_seed(0)\n",
    "            if last_exists:\n",
    "                model_saver.restore(sess, last_path)\n",
    "            else:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            train_writer = tf.summary.FileWriter(logs_dir + '/tb/train',\n",
    "                                                 sess.graph)\n",
    "            valid_writer = tf.summary.FileWriter(logs_dir + '/tb/valid')\n",
    "\n",
    "            total_batches = int(X_train.shape[0] // batch_size)\n",
    "\n",
    "            def _train_feed_dict(step):\n",
    "                begin = (step % total_batches) * batch_size\n",
    "                end = (step % total_batches + 1) * batch_size\n",
    "                return {\n",
    "                    self.x: X_train[begin:end],\n",
    "                    self.s: s_train[begin:end],\n",
    "                    self.y: y_train[begin:end],\n",
    "                    self.x_marg: X_train_marginal[begin:end],\n",
    "                    self.s_marg: s_train_marginal[begin:end],\n",
    "                    self.y_marg: y_train_marginal[begin:end],\n",
    "                    self.keep_prob: 1.0\n",
    "                }\n",
    "\n",
    "            for _ in range(n_iterations):\n",
    "                iteration = sess.run(self.inc_iteration)\n",
    "\n",
    "                for _ in range(pred_steps_per_iteration):\n",
    "                    step = sess.run(self.inc_step)\n",
    "                    s, _ = sess.run([self.summary_op, self.train_pred],\n",
    "                                    feed_dict=_train_feed_dict(step))\n",
    "                    train_writer.add_summary(s, step)\n",
    "\n",
    "                if iteration % report_iterations == 0:\n",
    "                    s = sess.run(\n",
    "                        self.summary_op,\n",
    "                        feed_dict={self.x: X_valid,\n",
    "                                   self.s: s_valid,\n",
    "                                   self.y: y_valid,\n",
    "                                   self.x_marg: X_valid_marginal,\n",
    "                                   self.s_marg: s_valid_marginal,\n",
    "                                   self.y_marg: y_valid_marginal,\n",
    "                                   self.keep_prob: 1.0})\n",
    "                    valid_writer.add_summary(s, step)\n",
    "\n",
    "                if iteration % model_save_iterations == 0:\n",
    "                    path = models_dir + 'iteration_{}.session'.format(iteration)\n",
    "                    model_saver.save(sess, path)\n",
    "\n",
    "                if verbose and iteration % report_iterations == 0:\n",
    "                    print(\"Finished iteration {}\".format(iteration))\n",
    "\n",
    "            model_saver.save(sess, last_path)\n",
    "\n",
    "    def predict(self, model, features, logs_dir_f, tf_config, iteration, SEED_NUM):\n",
    "        model_saver = tf.train.Saver()\n",
    "        with tf.Session(config=tf_config) as sess:\n",
    "            path = '{}/models_{}/iteration_{}.session'.format(logs_dir_f, SEED_NUM, iteration)\n",
    "            model_saver.restore(sess, path)\n",
    "            y_pred = model.y_pred.eval({model.x: features, model.keep_prob: 1.0})\n",
    "            y_prob = model.y_prob.eval({model.x: features, model.keep_prob: 1.0})\n",
    "            decoded = model.decoded.eval({model.x: features, model.keep_prob: 1.0})\n",
    "        return y_pred, y_prob, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_train, data_train_marginal, data_valid, data_valid_marginal, x_size, s_size, y_size, med_sq_dist,\n",
    "          features, logs_dir_f, SEED_NUM, model_config, fit_config, device=\"cpu\"):\n",
    "    kernel_mapper = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim=x_size,\n",
    "                                                                         output_dim=model_config['rff_samples'],\n",
    "                                                                         stddev=med_sq_dist, seed=888,\n",
    "                                                                         name='kernel_mapper')\n",
    "    kernel_mapper_sens = tf.contrib.kernel_methods.RandomFourierFeatureMapper(input_dim=s_size,\n",
    "                                                                              output_dim=model_config['rff_samples'],\n",
    "                                                                              stddev=1.0, seed=888,\n",
    "                                                                              name='kernel_mapper_sens')\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        device = \"/cpu:0\"\n",
    "    else:\n",
    "        device = \"/gpu:0\"\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Graph().as_default():\n",
    "        model = Model(features_size=x_size, protected_size=s_size, target_size=y_size, features_names=features,\n",
    "                      rff_map=kernel_mapper, rff_map_sens=kernel_mapper_sens, to_deploy=False, device=device,\n",
    "                      **model_config)\n",
    "\n",
    "        tf_config = tf.ConfigProto()\n",
    "        tf_config.gpu_options.allow_growth = True\n",
    "\n",
    "        model.fit(data_train, data_train_marginal, data_valid, data_valid_marginal, SEED_NUM,\n",
    "                  tf_config=tf_config, verbose=True, logs_dir=logs_dir_f,\n",
    "                  **fit_config)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_train, data_valid, data_test, features, logs_dir_f, SEED_NUM, model_config, device=\"cpu\"):\n",
    "    # Computational graphs are associated with Sessions. \n",
    "    # We should \"clear out\" the state of the Session so we don't have multiple placeholder objects floating around \n",
    "    # as we call save and restore()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        device = \"/cpu:0\"\n",
    "    else:\n",
    "        device = \"/gpu:0\"\n",
    "\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    kernel_mapper = None\n",
    "    kernel_mapper_sens = None\n",
    "    model = Model(features_size=data_train[0].shape[1], protected_size=data_train[1].shape[1],\n",
    "                  target_size=data_train[2].shape[1], features_names=features, rff_map=kernel_mapper,\n",
    "                  rff_map_sens=kernel_mapper_sens, to_deploy=True, device=device,\n",
    "                  **model_config)\n",
    "\n",
    "    all_iterations = []\n",
    "    for file in os.listdir(logs_dir_f + '/models_{}/'.format(SEED_NUM)):\n",
    "        if fnmatch.fnmatch(file, 'iteration_*.session.meta'):\n",
    "            iteration = file[len('iteration_'): -len('.session.meta')]\n",
    "            all_iterations.append(int(iteration))\n",
    "    all_iterations = sorted(all_iterations)\n",
    "\n",
    "    X_train, s_train, y_train = data_train\n",
    "    X_valid, s_valid, y_valid = data_valid\n",
    "    X_test, s_test, y_test = data_test\n",
    "    tpr_diff = []\n",
    "    fpr_dif = []\n",
    "    acc_ = []\n",
    "\n",
    "    # perform classification here with X and Xtilde\n",
    "    reg_array = [10 ** i for i in range(7)]\n",
    "    n_splits = 3\n",
    "    cv = cross_validation.StratifiedKFold(n_splits=n_splits, random_state=888, shuffle=True)\n",
    "    # with Xtilde\n",
    "    print(\"with Xtilde for all iterations\")\n",
    "    decoded_train = None\n",
    "    decoded_test = None\n",
    "    for iteration in all_iterations[-1:]:\n",
    "        y_pred_train, y_prob_train, decoded_train = model.predict(model, X_train, logs_dir_f, tf_config, iteration,\n",
    "                                                                  SEED_NUM)\n",
    "        y_pred_valid, y_prob_valid, decoded_valid = model.predict(model, X_valid, logs_dir_f, tf_config, iteration,\n",
    "                                                                  SEED_NUM)\n",
    "        y_pred_test, y_prob_test, decoded_test = model.predict(model, X_test, logs_dir_f, tf_config, iteration,\n",
    "                                                               SEED_NUM)\n",
    "\n",
    "        cv_scores = np.zeros((len(reg_array), n_splits))\n",
    "        for i, reg_const in enumerate(reg_array):\n",
    "            cv_scores[i] = cross_validation.cross_val_score(\n",
    "                svm.LinearSVC(C=reg_const, dual=False, tol=1e-6, random_state=888), decoded_train, y_train.flatten(),\n",
    "                cv=cv)\n",
    "        cv_mean = np.mean(cv_scores, axis=1)\n",
    "        reg_best = reg_array[np.argmax(cv_mean)]\n",
    "        print(\"Regularization \", reg_best)\n",
    "        clf = svm.LinearSVC(C=reg_best, dual=False, tol=1e-6, random_state=888)\n",
    "        clf.fit(decoded_train, y_train.flatten())\n",
    "\n",
    "        predictions = clf.predict(decoded_test)\n",
    "        # performance measurement\n",
    "        acc, acc_sensitive = compute_accuracy_pvalue(y_test.flatten(), predictions,\n",
    "                                                     s_test.flatten())\n",
    "        print('SVM Accuracy: %.2f%%' % (acc * 100.))\n",
    "        print(\"per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "        acc_sensitive[0] * 100., acc_sensitive[1] * 100., (acc_sensitive[0] - acc_sensitive[1]) * 100.))\n",
    "        fpr, fnr, fpr_fnr_tpr_sensitive = compute_fpr_fnr(y_test.flatten(), predictions,\n",
    "                                                          s_test.flatten())\n",
    "        print('SVM FPR and FNR: %.2f, %.2f' % (fpr * 100., fnr * 100.))\n",
    "        print(\"TPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "        fpr_fnr_tpr_sensitive[2, 0] * 100., fpr_fnr_tpr_sensitive[2, 1] * 100.,\n",
    "        (fpr_fnr_tpr_sensitive[2, 0] - fpr_fnr_tpr_sensitive[2, 1]) * 100.))\n",
    "        print(\"FPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "        fpr_fnr_tpr_sensitive[0, 0] * 100., fpr_fnr_tpr_sensitive[0, 1] * 100.,\n",
    "        (fpr_fnr_tpr_sensitive[0, 0] - fpr_fnr_tpr_sensitive[0, 1]) * 100.))\n",
    "        print(\"FNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "        fpr_fnr_tpr_sensitive[1, 0] * 100., fpr_fnr_tpr_sensitive[1, 1] * 100.,\n",
    "        (fpr_fnr_tpr_sensitive[1, 0] - fpr_fnr_tpr_sensitive[1, 1]) * 100.))\n",
    "        print(\"TNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "        fpr_fnr_tpr_sensitive[3, 0] * 100., fpr_fnr_tpr_sensitive[3, 1] * 100.,\n",
    "        (fpr_fnr_tpr_sensitive[3, 0] - fpr_fnr_tpr_sensitive[3, 1]) * 100.))\n",
    "        print(\"\\n\")\n",
    "        acc_.append(acc)\n",
    "        tpr_diff.append((fpr_fnr_tpr_sensitive[2, 0] - fpr_fnr_tpr_sensitive[2, 1]) * 100.)\n",
    "        fpr_dif.append((fpr_fnr_tpr_sensitive[0, 0] - fpr_fnr_tpr_sensitive[0, 1]) * 100.)\n",
    "\n",
    "        x_column_names = [\"age\",\n",
    "                          \"education-num\",\n",
    "                          \"capital-gain\",\n",
    "                          \"capital-loss\",\n",
    "                          \"hours-per-week\",\n",
    "                          \"workclass_Federal-gov\",\n",
    "                          \"workclass_Local-gov\",\n",
    "                          \"workclass_Never-worked\",\n",
    "                          \"workclass_Private\",\n",
    "                          \"workclass_Self-emp-inc\",\n",
    "                          \"workclass_Self-emp-not-inc\",\n",
    "                          \"workclass_State-gov\",\n",
    "                          \"workclass_Without-pay\",\n",
    "                          \"education_10th\",\n",
    "                          \"education_11th\",\n",
    "                          \"education_12th\",\n",
    "                          \"education_1st-4th\",\n",
    "                          \"education_5th-6th\",\n",
    "                          \"education_7th-8th\",\n",
    "                          \"education_9th\",\n",
    "                          \"education_Assoc-acdm\",\n",
    "                          \"education_Assoc-voc\",\n",
    "                          \"education_Bachelors\",\n",
    "                          \"education_Doctorate\",\n",
    "                          \"education_HS-grad\",\n",
    "                          \"education_Masters\",\n",
    "                          \"education_Preschool\",\n",
    "                          \"education_Prof-school\",\n",
    "                          \"education_Some-college\",\n",
    "                          \"marital-status_Divorced\",\n",
    "                          \"marital-status_Married-AF-spouse\",\n",
    "                          \"marital-status_Married-civ-spouse\",\n",
    "                          \"marital-status_Married-spouse-absent\",\n",
    "                          \"marital-status_Never-married\",\n",
    "                          \"marital-status_Separated\",\n",
    "                          \"marital-status_Widowed\",\n",
    "                          \"occupation_Adm-clerical\",\n",
    "                          \"occupation_Armed-Forces\",\n",
    "                          \"occupation_Craft-repair\",\n",
    "                          \"occupation_Exec-managerial\",\n",
    "                          \"occupation_Farming-fishing\",\n",
    "                          \"occupation_Handlers-cleaners\",\n",
    "                          \"occupation_Machine-op-inspct\",\n",
    "                          \"occupation_Other-service\",\n",
    "                          \"occupation_Priv-house-serv\",\n",
    "                          \"occupation_Prof-specialty\",\n",
    "                          \"occupation_Protective-serv\",\n",
    "                          \"occupation_Sales\",\n",
    "                          \"occupation_Tech-support\",\n",
    "                          \"occupation_Transport-moving\",\n",
    "                          \"relationship_Husband\",\n",
    "                          \"relationship_Not-in-family\",\n",
    "                          \"relationship_Other-relative\",\n",
    "                          \"relationship_Own-child\",\n",
    "                          \"relationship_Unmarried\",\n",
    "                          \"relationship_Wife\",\n",
    "                          # \"sex_Female\",\n",
    "                          # \"sex_Male\",\n",
    "                          \"race_Amer-Indian-Eskimo\",\n",
    "                          \"race_Asian-Pac-Islander\",\n",
    "                          \"race_Black\",\n",
    "                          \"race_Other\",\n",
    "                          \"race_White\",\n",
    "                          \"native-country_Cambodia\",\n",
    "                          \"native-country_Canada\",\n",
    "                          \"native-country_China\",\n",
    "                          \"native-country_Columbia\",\n",
    "                          \"native-country_Cuba\",\n",
    "                          \"native-country_Dominican-Republic\",\n",
    "                          \"native-country_Ecuador\",\n",
    "                          \"native-country_El-Salvador\",\n",
    "                          \"native-country_England\",\n",
    "                          \"native-country_France\",\n",
    "                          \"native-country_Germany\",\n",
    "                          \"native-country_Greece\",\n",
    "                          \"native-country_Guatemala\",\n",
    "                          \"native-country_Haiti\",\n",
    "                          \"native-country_Holand-Netherlands\",\n",
    "                          \"native-country_Honduras\",\n",
    "                          \"native-country_Hong\",\n",
    "                          \"native-country_Hungary\",\n",
    "                          \"native-country_India\",\n",
    "                          \"native-country_Iran\",\n",
    "                          \"native-country_Ireland\",\n",
    "                          \"native-country_Italy\",\n",
    "                          \"native-country_Jamaica\",\n",
    "                          \"native-country_Japan\",\n",
    "                          \"native-country_Laos\",\n",
    "                          \"native-country_Mexico\",\n",
    "                          \"native-country_Nicaragua\",\n",
    "                          \"native-country_Outlying-US(Guam-USVI-etc)\",\n",
    "                          \"native-country_Peru\",\n",
    "                          \"native-country_Philippines\",\n",
    "                          \"native-country_Poland\",\n",
    "                          \"native-country_Portugal\",\n",
    "                          \"native-country_Puerto-Rico\",\n",
    "                          \"native-country_Scotland\",\n",
    "                          \"native-country_South\",\n",
    "                          \"native-country_Taiwan\",\n",
    "                          \"native-country_Thailand\",\n",
    "                          \"native-country_Trinadad&Tobago\",\n",
    "                          \"native-country_United-States\",\n",
    "                          \"native-country_Vietnam\",\n",
    "                          \"native-country_Yugoslavia\",\n",
    "                          ]\n",
    "\n",
    "        s_column_name = [\"sensitive\"]\n",
    "        y_column_name = [\"label\"]\n",
    "\n",
    "        train_x_dataframe = pd.DataFrame(X_train)\n",
    "        train_s_dataframe = pd.DataFrame(s_train, dtype='int32')\n",
    "        train_y_dataframe = pd.DataFrame(y_train, dtype='int32')\n",
    "\n",
    "        train_x_dataframe.columns = x_column_names\n",
    "        train_s_dataframe.columns = s_column_name\n",
    "        train_y_dataframe.columns = y_column_name\n",
    "\n",
    "        train_dataframe = pd.concat([train_x_dataframe, train_s_dataframe, train_y_dataframe], axis=1)\n",
    "\n",
    "        train_x_tilde_dataframe = pd.DataFrame(decoded_train)\n",
    "        train_x_tilde_dataframe.columns = x_column_names\n",
    "        train_tilde_dataframe = pd.concat([train_x_tilde_dataframe, train_s_dataframe, train_y_dataframe], axis=1)\n",
    "\n",
    "        train_dataframe.to_csv(\"seed_{}_stylingtrain_{}.csv\".format(SEED_NUM, iteration), index=False)\n",
    "        train_tilde_dataframe.to_csv(\"seed_{}_stylingtraintilde_{}.csv\".format(SEED_NUM, iteration), index=False)\n",
    "\n",
    "        test_x_dataframe = pd.DataFrame(X_test)\n",
    "        test_s_dataframe = pd.DataFrame(s_test, dtype='int32')\n",
    "        test_y_dataframe = pd.DataFrame(y_test, dtype='int32')\n",
    "\n",
    "        test_x_dataframe.columns = x_column_names\n",
    "        test_s_dataframe.columns = s_column_name\n",
    "        test_y_dataframe.columns = y_column_name\n",
    "\n",
    "        test_dataframe = pd.concat([test_x_dataframe, test_s_dataframe, test_y_dataframe], axis=1)\n",
    "\n",
    "        test_x_tilde_dataframe = pd.DataFrame(decoded_test)\n",
    "        test_x_tilde_dataframe.columns = x_column_names\n",
    "        test_tilde_dataframe = pd.concat([test_x_tilde_dataframe, test_s_dataframe, test_y_dataframe], axis=1)\n",
    "\n",
    "        test_dataframe.to_csv(\"seed_{}_stylingtest_{}.csv\".format(SEED_NUM, iteration), index=False)\n",
    "        test_tilde_dataframe.to_csv(\"seed_{}_stylingtesttilde_{}.csv\".format(SEED_NUM, iteration), index=False)\n",
    "\n",
    "    print(np.array(acc_))\n",
    "    print(np.array(tpr_diff))\n",
    "    print(np.array(fpr_dif))\n",
    "\n",
    "    # performance measurement\n",
    "    cv_scores = np.zeros((len(reg_array), n_splits))\n",
    "    print(\"with X\")\n",
    "    for i, reg_const in enumerate(reg_array):\n",
    "        cv_scores[i] = cross_validation.cross_val_score(\n",
    "            svm.LinearSVC(C=reg_const, dual=False, tol=1e-6, random_state=888), X_train, y_train.flatten(), cv=cv)\n",
    "    print(\"CV_Scores\", cv_scores)\n",
    "    cv_mean = np.mean(cv_scores, axis=1)\n",
    "    print(\"CV Mean\", cv_mean)\n",
    "    reg_best = reg_array[np.argmax(cv_mean)]\n",
    "    clf = svm.LinearSVC(C=reg_best, dual=False, tol=1e-6, random_state=888)\n",
    "    clf.fit(X_train, y_train.flatten())\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    acc, acc_sensitive = compute_accuracy_pvalue(y_test.flatten(), predictions,\n",
    "                                                 s_test.flatten())\n",
    "    print('SVM Accuracy: %.2f%%' % (acc * 100.))\n",
    "    print('Reg: %.2f' % (reg_best))\n",
    "    print(\"per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    acc_sensitive[0] * 100., acc_sensitive[1] * 100., (acc_sensitive[0] - acc_sensitive[1]) * 100.))\n",
    "    fpr, fnr, fpr_fnr_tpr_sensitive = compute_fpr_fnr(y_test.flatten(), predictions,\n",
    "                                                      s_test.flatten())\n",
    "    print('SVM FPR and FNR: %.2f, %.2f' % (fpr * 100., fnr * 100.))\n",
    "    print(\"TPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[2, 0] * 100., fpr_fnr_tpr_sensitive[2, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[2, 0] - fpr_fnr_tpr_sensitive[2, 1]) * 100.))\n",
    "    print(\"FPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[0, 0] * 100., fpr_fnr_tpr_sensitive[0, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[0, 0] - fpr_fnr_tpr_sensitive[0, 1]) * 100.))\n",
    "    print(\"FNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[1, 0] * 100., fpr_fnr_tpr_sensitive[1, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[1, 0] - fpr_fnr_tpr_sensitive[1, 1]) * 100.))\n",
    "    print(\"TNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[3, 0] * 100., fpr_fnr_tpr_sensitive[3, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[3, 0] - fpr_fnr_tpr_sensitive[3, 1]) * 100.))\n",
    "    print(\"\\n\")\n",
    "    # with Xtilde\n",
    "    cv_scores = np.zeros((len(reg_array), n_splits))\n",
    "    print(\"with Xtilde\")\n",
    "    for i, reg_const in enumerate(reg_array):\n",
    "        cv_scores[i] = cross_validation.cross_val_score(\n",
    "            svm.LinearSVC(C=reg_const, dual=False, tol=1e-6, random_state=888), decoded_train, y_train.flatten(), cv=cv)\n",
    "    print(\"CV_Scores\", cv_scores)\n",
    "    cv_mean = np.mean(cv_scores, axis=1)\n",
    "    print(\"CV Mean\", cv_mean)\n",
    "    reg_best = reg_array[np.argmax(cv_mean)]\n",
    "    clf = svm.LinearSVC(C=reg_best, dual=False, tol=1e-6, random_state=888)\n",
    "    clf.fit(decoded_train, y_train.flatten())\n",
    "    predictions = clf.predict(decoded_test)\n",
    "    # performance measurement\n",
    "    acc, acc_sensitive = compute_accuracy_pvalue(y_test.flatten(), predictions,\n",
    "                                                 s_test.flatten())\n",
    "    print('SVM Accuracy: %.2f%%' % (acc * 100.))\n",
    "    print(\"reg value: %.2f\" % (reg_best))\n",
    "    print(\"per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    acc_sensitive[0] * 100., acc_sensitive[1] * 100., (acc_sensitive[0] - acc_sensitive[1]) * 100.))\n",
    "    fpr, fnr, fpr_fnr_tpr_sensitive = compute_fpr_fnr(y_test.flatten(), predictions,\n",
    "                                                      s_test.flatten())\n",
    "    print('SVM FPR and FNR: %.2f, %.2f' % (fpr * 100., fnr * 100.))\n",
    "    print(\"TPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[2, 0] * 100., fpr_fnr_tpr_sensitive[2, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[2, 0] - fpr_fnr_tpr_sensitive[2, 1]) * 100.))\n",
    "    print(\"FPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[0, 0] * 100., fpr_fnr_tpr_sensitive[0, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[0, 0] - fpr_fnr_tpr_sensitive[0, 1]) * 100.))\n",
    "    print(\"FNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[1, 0] * 100., fpr_fnr_tpr_sensitive[1, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[1, 0] - fpr_fnr_tpr_sensitive[1, 1]) * 100.))\n",
    "    print(\"TNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[3, 0] * 100., fpr_fnr_tpr_sensitive[3, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[3, 0] - fpr_fnr_tpr_sensitive[3, 1]) * 100.))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    acc, acc_sensitive = compute_accuracy_pvalue(y_test.flatten(), y_pred_test.flatten(), s_test.flatten())\n",
    "    print('Encoder Accuracy: %.2f%%' % (acc * 100.))\n",
    "    print(\"per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    acc_sensitive[0] * 100., acc_sensitive[1] * 100., (acc_sensitive[0] - acc_sensitive[1]) * 100.))\n",
    "    fpr, fnr, fpr_fnr_tpr_sensitive = compute_fpr_fnr(y_test.flatten(), y_pred_test.flatten(),\n",
    "                                                      s_test.flatten())\n",
    "    print('Encoder FPR and FNR: %.2f, %.2f' % (fpr * 100., fnr * 100.))\n",
    "    print(\"TPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[2, 0] * 100., fpr_fnr_tpr_sensitive[2, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[2, 0] - fpr_fnr_tpr_sensitive[2, 1]) * 100.))\n",
    "    print(\"FPR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[0, 0] * 100., fpr_fnr_tpr_sensitive[0, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[0, 0] - fpr_fnr_tpr_sensitive[0, 1]) * 100.))\n",
    "    print(\"FNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[1, 0] * 100., fpr_fnr_tpr_sensitive[1, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[1, 0] - fpr_fnr_tpr_sensitive[1, 1]) * 100.))\n",
    "    print(\"TNR per sensitive value: %.2f, %.2f, (%.2f)\" % (\n",
    "    fpr_fnr_tpr_sensitive[3, 0] * 100., fpr_fnr_tpr_sensitive[3, 1] * 100.,\n",
    "    (fpr_fnr_tpr_sensitive[3, 0] - fpr_fnr_tpr_sensitive[3, 1]) * 100.))\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_models(seed):\n",
    "    models_dir = './models_{}/'.format(seed)\n",
    "    if os.path.exists(models_dir):\n",
    "        import shutil\n",
    "        shutil.rmtree(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(seed):\n",
    "    file_dir = '.'\n",
    "    for f in os.listdir(file_dir):\n",
    "        if re.search(r\".*\\.csv\", f):\n",
    "            os.remove(os.path.join(file_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/Categories_Overall.xlsx', dtype=object)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b33f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Search Topic', axis=1)\n",
    "data = data.drop('Label', axis=1)\n",
    "data = data.drop('Ad ID', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02542e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
